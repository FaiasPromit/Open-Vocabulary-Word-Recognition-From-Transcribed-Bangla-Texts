{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Cite this paper if this code helps you:\n",
        "F. Satter and S. M. Masudul Ahsan, \"Open Vocabulary Word Recognition From Transcribed Bangla Texts,\" 2023 26th International Conference on Computer and Information Technology (ICCIT), Cox's Bazar, Bangladesh, 2023, pp. 1-6, doi: 10.1109/ICCIT60459.2023.10441393."
      ],
      "metadata": {
        "id": "eg8G8vYEPgvd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baIucqxDYrBu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aC2HV9xY3Rv",
        "outputId": "a12fee3b-187c-4651-babb-9874a34e4a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# # this creates a symbolic link so that now the path /content/gdrive/My Drive/ is equal to /mydrive\n",
        "\n",
        "# !ln -s /content/gdrive/My Drive/ /mydrive\n",
        "# !ls /mydrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkRCz10JY4UV",
        "outputId": "957514e6-bacd-4c59-9143-c96e7beefa27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/models (1) (1)/research\n",
            "Processing /content/gdrive/MyDrive/models (1) (1)/research\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting avro-python3 (from object-detection==0.1)\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting apache-beam (from object-detection==0.1)\n",
            "  Downloading apache_beam-2.49.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (9.4.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (4.9.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (3.7.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (0.29.36)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (21.6.0)\n",
            "Requirement already satisfied: tf-slim in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.16.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.0.7)\n",
            "Collecting lvis (from object-detection==0.1)\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.10.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (1.5.3)\n",
            "Collecting tf-models-official>=2.5.1 (from object-detection==0.1)\n",
            "  Downloading tf_models_official-2.13.1-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow_io (from object-detection==0.1)\n",
            "  Downloading tensorflow_io-0.33.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (28.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.6/28.6 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from object-detection==0.1) (2.12.0)\n",
            "Collecting pyparsing==2.4.7 (from object-detection==0.1)\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu<=2.2.0 (from object-detection==0.1)\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.6/116.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu<=2.2.0->object-detection==0.1)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (2023.6.3)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu<=2.2.0->object-detection==0.1) (1.23.5)\n",
            "Collecting colorama (from sacrebleu<=2.2.0->object-detection==0.1)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.6.7 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (2.84.0)\n",
            "Collecting immutabledict (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading immutabledict-3.0.0-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: kaggle>=1.3.9 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (1.5.16)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.1.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.8.0.76)\n",
            "Requirement already satisfied: psutil>=5.4.3 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (9.0.0)\n",
            "Collecting pyyaml<5.4.0,>=5.1 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (4.9.2)\n",
            "Requirement already satisfied: tensorflow-hub>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-models-official>=2.5.1->object-detection==0.1) (0.14.0)\n",
            "Collecting tensorflow-model-optimization>=0.4.1 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading tensorflow_model_optimization-0.7.5-py2.py3-none-any.whl (241 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-text~=2.13.0 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading tensorflow_text-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow~=2.13.0 (from tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->object-detection==0.1) (2023.3)\n",
            "Requirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from tf-slim->object-detection==0.1) (1.4.0)\n",
            "Collecting crcmod<2.0,>=1.7 (from apache-beam->object-detection==0.1)\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.7/89.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting orjson<4.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading orjson-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1 (from apache-beam->object-detection==0.1)\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.0/152.0 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cloudpickle~=2.2.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.2.1)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam->object-detection==0.1)\n",
            "  Downloading fastavro-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m65.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasteners<1.0,>=0.3 (from apache-beam->object-detection==0.1)\n",
            "  Downloading fasteners-0.18-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.57.0)\n",
            "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading hdfs-2.7.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.4/43.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (0.22.0)\n",
            "Collecting objsize<0.7.0,>=0.6.1 (from apache-beam->object-detection==0.1)\n",
            "  Downloading objsize-0.6.1-py3-none-any.whl (9.3 kB)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading pymongo-4.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m671.3/671.3 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.22.3)\n",
            "Requirement already satisfied: protobuf<4.24.0,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (3.20.3)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (1.4.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (4.7.1)\n",
            "Collecting zstandard<1,>=0.18.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading zstandard-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow<12.0.0,>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from apache-beam->object-detection==0.1) (9.0.0)\n",
            "Requirement already satisfied: cycler>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (1.4.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.0.25 in /usr/local/lib/python3.10/dist-packages (from lvis->object-detection==0.1) (4.8.0.76)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (1.1.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (4.42.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->object-detection==0.1) (23.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.33.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_io->object-detection==0.1) (0.33.0)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.17.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (0.1.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (2.11.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (4.1.1)\n",
            "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam->object-detection==0.1)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2023.7.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (2.0.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (6.0.0)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam->object-detection==0.1)\n",
            "  Downloading dnspython-2.4.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam->object-detection==0.1) (3.4)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (3.9.0)\n",
            "Collecting keras (from object-detection==0.1)\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (67.7.2)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1)\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (2.3.0)\n",
            "Collecting typing-extensions>=3.7.0 (from apache-beam->object-detection==0.1)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (1.14.1)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-model-optimization>=0.4.1->tf-models-official>=2.5.1->object-detection==0.1) (0.1.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client->tf-models-official>=2.5.1->object-detection==0.1) (4.9)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.2.2)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (8.1.7)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.4.1)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (1.14.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (0.10.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (0.41.2)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (6.0.1)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets->tf-models-official>=2.5.1->object-detection==0.1) (3.16.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (1.60.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.6.7->tf-models-official>=2.5.1->object-detection==0.1) (5.3.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official>=2.5.1->object-detection==0.1) (3.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (2.3.7)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle>=1.3.9->tf-models-official>=2.5.1->object-detection==0.1) (1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (2.1.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow~=2.13.0->tf-models-official>=2.5.1->object-detection==0.1) (3.2.2)\n",
            "Building wheels for collected packages: object-detection, avro-python3, crcmod, dill, hdfs, pyyaml, seqeval, docopt\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1697182 sha256=e3f5af16bb676532405e8849317576380eead34e2a173cd1657de2af2b01ddf6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-tol7_fum/wheels/4b/2a/a2/2c0750af9304f445650c100176a216019c82cac62fc39531e7\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=43991 sha256=102e3bf17791260beb6729134e5412cc09007ccbecc05fe8ad446a0e551a1fb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/85/62/6cdd81c56f923946b401cecff38055b94c9b766927f7d8ca82\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=31406 sha256=17a8593830efccefa7d42f05b2fbc973f97aa82a0fe6648d0c8e99265d825e67\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78542 sha256=8d46a9f1161361e4f5158d5ef4dee0efc3ad819aceaf189cadf5c67b627bd902\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
            "  Building wheel for hdfs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdfs: filename=hdfs-2.7.2-py3-none-any.whl size=34168 sha256=0d34055ab3ac764ed092f690485cb9f00b5d396365654924289d7548cf2db6b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/39/8e/e1905de9af8ae74911cd3e53e721995cd230816f63776e5825\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp310-cp310-linux_x86_64.whl size=44635 sha256=546bba8b9e7cb983fdfcc1bd02558e27bfae0aef47e75539be9aaef955428b8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/a9/6a/d0a6981a8dbb698845178818642f72ce179f14336908c7df01\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=4d0b9025518407da922f9a0e8170230b1da5db27a58913a70131336fd242baf1\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13705 sha256=e6bb9d7bc0ff53ab3ba0bdc518e23b412f163b5f79a2dfaf7188c0b49a65f59c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built object-detection avro-python3 crcmod dill hdfs pyyaml seqeval docopt\n",
            "Installing collected packages: sentencepiece, docopt, crcmod, zstandard, typing-extensions, tensorflow-model-optimization, tensorflow_io, tensorflow-estimator, pyyaml, pyparsing, portalocker, orjson, objsize, keras, immutabledict, fasteners, fastavro, dnspython, dill, colorama, avro-python3, sacrebleu, pymongo, hdfs, seqeval, lvis, apache-beam, tensorboard, tensorflow, tensorflow-text, tf-models-official, object-detection\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.1\n",
            "    Uninstalling PyYAML-6.0.1:\n",
            "      Successfully uninstalled PyYAML-6.0.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "flax 0.7.2 requires PyYAML>=5.4.1, but you have pyyaml 5.3.1 which is incompatible.\n",
            "pydantic 2.2.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.6.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed apache-beam-2.49.0 avro-python3-1.10.2 colorama-0.4.6 crcmod-1.7 dill-0.3.1.1 dnspython-2.4.2 docopt-0.6.2 fastavro-1.8.2 fasteners-0.18 hdfs-2.7.2 immutabledict-3.0.0 keras-2.13.1 lvis-0.5.3 object-detection-0.1 objsize-0.6.1 orjson-3.9.5 portalocker-2.7.0 pymongo-4.5.0 pyparsing-2.4.7 pyyaml-5.3.1 sacrebleu-2.2.0 sentencepiece-0.1.99 seqeval-1.2.2 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-model-optimization-0.7.5 tensorflow-text-2.13.0 tensorflow_io-0.33.0 tf-models-official-2.13.1 typing-extensions-4.5.0 zstandard-0.21.0\n"
          ]
        }
      ],
      "source": [
        "# clone the tensorflow models on the colab cloud vm\n",
        "\n",
        "# !git clone --q https://github.com/tensorflow/models.git\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# navigate to /models/research folder to compile protos\n",
        "\n",
        "# update ashche git e. ekhon ar ager code kaaj kore na. tai 2/3 ta ager commit download kore drive e up disi.  oita models 1. oita use kortesi. models e ache satter1707116 er shortcut\n",
        "\n",
        "%cd /content/gdrive/MyDrive/models (1)/research\n",
        "\n",
        "# Compile protos.\n",
        "\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "\n",
        "# Install TensorFlow Object Detection API.\n",
        "\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!python -m pip install .\n",
        "#ekhane ekta . deya lagse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW1rRsabY5uI",
        "outputId": "3a368273-ef0f-4696-ba64-5260184917b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-26 18:21:54.134041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running tests under Python 3.10.12: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "WARNING:tensorflow:`tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
            "W0826 18:22:01.424709 137125554692096 batch_normalization.py:1531] `tf.keras.layers.experimental.SyncBatchNormalization` endpoint is deprecated and will be removed in a future release. Please use `tf.keras.layers.BatchNormalization` with parameter `synchronized` set to True.\n",
            "W0826 18:22:03.100185 137125554692096 model_builder.py:1112] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 3.94s\n",
            "I0826 18:22:04.719144 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 3.94s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 3.38s\n",
            "I0826 18:22:08.105247 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 3.38s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 2.42s\n",
            "I0826 18:22:10.538826 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 2.42s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 2.89s\n",
            "I0826 18:22:13.431672 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 2.89s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 13.6s\n",
            "I0826 18:22:27.031688 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 13.6s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I0826 18:22:27.051248 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.18s\n",
            "I0826 18:22:27.242846 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.18s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.1s\n",
            "I0826 18:22:27.341820 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.1s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.11s\n",
            "I0826 18:22:27.463149 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.11s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.83s\n",
            "I0826 18:22:28.292937 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.83s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.8s\n",
            "I0826 18:22:29.093060 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.8s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 1.14s\n",
            "I0826 18:22:30.238858 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 1.14s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.65s\n",
            "I0826 18:22:30.894158 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.65s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.77s\n",
            "I0826 18:22:31.674597 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.77s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.25s\n",
            "I0826 18:22:31.933623 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.25s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I0826 18:22:33.145228 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I0826 18:22:33.146532 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 64\n",
            "I0826 18:22:33.146673 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 3\n",
            "I0826 18:22:33.168497 137125554692096 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0826 18:22:33.365010 137125554692096 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0826 18:22:33.365339 137125554692096 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0826 18:22:33.916523 137125554692096 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0826 18:22:33.916853 137125554692096 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0826 18:22:35.228792 137125554692096 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0826 18:22:35.231314 137125554692096 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0826 18:22:36.830001 137125554692096 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0826 18:22:36.830606 137125554692096 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0826 18:22:37.781064 137125554692096 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0826 18:22:37.781393 137125554692096 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0826 18:22:38.864558 137125554692096 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0826 18:22:38.864875 137125554692096 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0826 18:22:41.963215 137125554692096 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0826 18:22:41.970313 137125554692096 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0826 18:22:42.471094 137125554692096 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0826 18:22:42.621282 137125554692096 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0826 18:22:42.753683 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I0826 18:22:42.753948 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 88\n",
            "I0826 18:22:42.754036 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\n",
            "I0826 18:22:42.757844 137125554692096 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0826 18:22:42.794383 137125554692096 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0826 18:22:42.794624 137125554692096 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0826 18:22:43.098319 137125554692096 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0826 18:22:43.098562 137125554692096 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0826 18:22:43.642160 137125554692096 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0826 18:22:43.642431 137125554692096 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0826 18:22:44.173844 137125554692096 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I0826 18:22:44.174087 137125554692096 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0826 18:22:45.425216 137125554692096 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I0826 18:22:45.425522 137125554692096 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0826 18:22:46.169440 137125554692096 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I0826 18:22:46.169690 137125554692096 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0826 18:22:47.261099 137125554692096 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I0826 18:22:47.261470 137125554692096 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I0826 18:22:47.800667 137125554692096 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I0826 18:22:47.915830 137125554692096 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0826 18:22:48.102536 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I0826 18:22:48.102836 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 112\n",
            "I0826 18:22:48.102933 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 5\n",
            "I0826 18:22:48.106787 137125554692096 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0826 18:22:48.141264 137125554692096 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I0826 18:22:48.141503 137125554692096 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0826 18:22:48.421658 137125554692096 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I0826 18:22:48.421972 137125554692096 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0826 18:22:49.134641 137125554692096 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I0826 18:22:49.134940 137125554692096 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0826 18:22:49.814325 137125554692096 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0826 18:22:49.814610 137125554692096 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I0826 18:22:50.637791 137125554692096 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I0826 18:22:50.638084 137125554692096 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I0826 18:22:51.419930 137125554692096 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I0826 18:22:51.420197 137125554692096 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I0826 18:22:52.613657 137125554692096 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I0826 18:22:52.613957 137125554692096 efficientnet_model.py:143] round_filter input=320 output=352\n",
            "I0826 18:22:53.167609 137125554692096 efficientnet_model.py:143] round_filter input=1280 output=1408\n",
            "I0826 18:22:53.291795 137125554692096 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0826 18:22:53.434696 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I0826 18:22:53.434974 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 160\n",
            "I0826 18:22:53.435087 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 6\n",
            "I0826 18:22:53.439107 137125554692096 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I0826 18:22:53.485465 137125554692096 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I0826 18:22:53.485716 137125554692096 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0826 18:22:53.842863 137125554692096 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0826 18:22:53.843221 137125554692096 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0826 18:22:54.513509 137125554692096 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0826 18:22:54.513836 137125554692096 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0826 18:22:55.129156 137125554692096 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I0826 18:22:55.129474 137125554692096 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I0826 18:22:56.150157 137125554692096 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I0826 18:22:56.150450 137125554692096 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I0826 18:22:57.204972 137125554692096 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I0826 18:22:57.205302 137125554692096 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I0826 18:22:58.690842 137125554692096 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I0826 18:22:58.691143 137125554692096 efficientnet_model.py:143] round_filter input=320 output=384\n",
            "I0826 18:22:59.295323 137125554692096 efficientnet_model.py:143] round_filter input=1280 output=1536\n",
            "I0826 18:22:59.452290 137125554692096 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0826 18:22:59.570771 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I0826 18:22:59.570974 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 224\n",
            "I0826 18:22:59.571034 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 7\n",
            "I0826 18:22:59.573562 137125554692096 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0826 18:22:59.602257 137125554692096 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0826 18:22:59.602498 137125554692096 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0826 18:22:59.796846 137125554692096 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0826 18:22:59.797052 137125554692096 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0826 18:23:00.286458 137125554692096 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I0826 18:23:00.286689 137125554692096 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I0826 18:23:00.791385 137125554692096 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I0826 18:23:00.791589 137125554692096 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I0826 18:23:02.006025 137125554692096 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I0826 18:23:02.006330 137125554692096 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I0826 18:23:02.925731 137125554692096 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I0826 18:23:02.925987 137125554692096 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I0826 18:23:04.206695 137125554692096 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I0826 18:23:04.206930 137125554692096 efficientnet_model.py:143] round_filter input=320 output=448\n",
            "I0826 18:23:04.611395 137125554692096 efficientnet_model.py:143] round_filter input=1280 output=1792\n",
            "I0826 18:23:04.705712 137125554692096 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0826 18:23:04.814486 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I0826 18:23:04.814759 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 288\n",
            "I0826 18:23:04.814825 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 7\n",
            "I0826 18:23:04.817477 137125554692096 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0826 18:23:04.846467 137125554692096 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I0826 18:23:04.846705 137125554692096 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0826 18:23:05.179826 137125554692096 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I0826 18:23:05.180077 137125554692096 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0826 18:23:05.881199 137125554692096 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0826 18:23:05.881458 137125554692096 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I0826 18:23:06.531921 137125554692096 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I0826 18:23:06.532132 137125554692096 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I0826 18:23:07.465700 137125554692096 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I0826 18:23:07.465906 137125554692096 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I0826 18:23:08.458364 137125554692096 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I0826 18:23:08.458573 137125554692096 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I0826 18:23:10.196125 137125554692096 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I0826 18:23:10.196403 137125554692096 efficientnet_model.py:143] round_filter input=320 output=512\n",
            "I0826 18:23:11.210051 137125554692096 efficientnet_model.py:143] round_filter input=1280 output=2048\n",
            "I0826 18:23:11.379002 137125554692096 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0826 18:23:11.588356 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I0826 18:23:11.588666 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 384\n",
            "I0826 18:23:11.588760 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 8\n",
            "I0826 18:23:11.592952 137125554692096 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I0826 18:23:11.635523 137125554692096 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I0826 18:23:11.635820 137125554692096 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0826 18:23:12.121469 137125554692096 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0826 18:23:12.121781 137125554692096 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0826 18:23:13.283008 137125554692096 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I0826 18:23:13.283321 137125554692096 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I0826 18:23:14.521376 137125554692096 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I0826 18:23:14.521715 137125554692096 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I0826 18:23:16.302784 137125554692096 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I0826 18:23:16.303123 137125554692096 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I0826 18:23:18.746368 137125554692096 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I0826 18:23:18.746702 137125554692096 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I0826 18:23:21.537171 137125554692096 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I0826 18:23:21.537448 137125554692096 efficientnet_model.py:143] round_filter input=320 output=576\n",
            "I0826 18:23:22.347946 137125554692096 efficientnet_model.py:143] round_filter input=1280 output=2304\n",
            "I0826 18:23:22.461174 137125554692096 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I0826 18:23:22.610613 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I0826 18:23:22.610813 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:152] EfficientDet BiFPN num filters: 384\n",
            "I0826 18:23:22.610903 137125554692096 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 8\n",
            "I0826 18:23:22.613378 137125554692096 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I0826 18:23:22.640524 137125554692096 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I0826 18:23:22.640756 137125554692096 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0826 18:23:23.058805 137125554692096 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I0826 18:23:23.059004 137125554692096 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I0826 18:23:23.967942 137125554692096 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I0826 18:23:23.968273 137125554692096 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I0826 18:23:24.902861 137125554692096 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I0826 18:23:24.903138 137125554692096 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I0826 18:23:26.351751 137125554692096 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I0826 18:23:26.352016 137125554692096 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I0826 18:23:27.860651 137125554692096 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I0826 18:23:27.860908 137125554692096 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I0826 18:23:30.297530 137125554692096 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I0826 18:23:30.297911 137125554692096 efficientnet_model.py:143] round_filter input=320 output=640\n",
            "I0826 18:23:31.914988 137125554692096 efficientnet_model.py:143] round_filter input=1280 output=2560\n",
            "I0826 18:23:32.130869 137125554692096 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 60.48s\n",
            "I0826 18:23:32.415743 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 60.48s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.01s\n",
            "I0826 18:23:32.503219 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.01s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I0826 18:23:32.506887 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I0826 18:23:32.509000 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I0826 18:23:32.512310 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I0826 18:23:32.515075 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I0826 18:23:32.515879 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I0826 18:23:32.517751 137125554692096 test_util.py:2462] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 24 tests in 91.751s\n",
            "\n",
            "OK (skipped=1)\n"
          ]
        }
      ],
      "source": [
        "!python object_detection/builders/model_builder_tf2_test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9mdWX-NY9pI"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "%matplotlib inline\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMSGRaDBY-pS",
        "outputId": "d45dbba4-ccb5-49ec-a8d7-03485763c3ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93\n",
            "93\n"
          ]
        }
      ],
      "source": [
        "classes=['blank','অ','ই','ঈ','উ','ঊ','ঋ','এ','ঐ','ও','ঔ','ক','খ','গ','ঘ','ঙ','চ','ছ','জ','ঝ','ঞ','ট','ঠ','ড','ঢ','ণ','ত','থ','দ','ধ','ন','প','ফ','ব','ভ','ম','য','র','ল','শ','ষ','স','হ','ড়','ঢ়','য়','ৎ','ঃ','ং','ঁ','০','১','২','৩','৪','৫','৬','৭','৮','৯','া','ি','ী','ে','ু','faka','্র','্য','ক্ষ','ন্ত','ত্র','ঙ্গ','স্থ','স্ব','ক্ত','স্ত','ন্দ','চ্ছ','দ্ধ','ন্ত্র','ফাকা','ত্ত','ষ্ট','ন্ন','ল্প','ম্প','faka','ূ','ৃ','ৈ','faka','ৌ','।']\n",
        "labels=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92]\n",
        "print(len(classes))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main steps - including (7)NMS and (8)PNO"
      ],
      "metadata": {
        "id": "1fvCQJJdMNAq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIWOsW6tZCDy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Object Detection (On Image) From TF2 Saved Model\n",
        "=====================================\n",
        "\"\"\"\n",
        "final_Lekha = []\n",
        "def detect_frame(frame,IMAGE_PATHS,isRealTime = False):\n",
        "    # def detect_frame(frame,isRealTime = False):\n",
        "    import os\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
        "    import pathlib\n",
        "    import tensorflow as tf\n",
        "    import cv2\n",
        "    import argparse\n",
        "    import numpy as np\n",
        "    from google.colab.patches import cv2_imshow\n",
        "\n",
        "    # Enable GPU dynamic memory allocation\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "    # PROVIDE PATH TO IMAGE DIRECTORY\n",
        "    print(IMAGE_PATHS)\n",
        "    IMAGE_PATHS = f'{IMAGE_PATHS}'\n",
        "    # frame = cv2.imread((\"/content/gdrive/MyDrive/customTF2/data/images/0.bmp\"))\n",
        "\n",
        "    # IMAGE_PATHS= frame\n",
        "    image_np = np.array(frame)\n",
        "    # PROVIDE PATH TO MODEL DIRECTORY\n",
        "    PATH_TO_MODEL_DIR = '/content/gdrive/MyDrive/customTF2/data/inference_graph (1)'\n",
        "\n",
        "    # PROVIDE PATH TO LABEL MAP\n",
        "    PATH_TO_LABELS = '/content/gdrive/MyDrive/customTF2/data/label_map.pbtxt'\n",
        "\n",
        "    # PROVIDE THE MINIMUM CONFIDENCE THRESHOLD\n",
        "    MIN_CONF_THRESH = float(0.20)\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "\n",
        "    import time\n",
        "    from object_detection.utils import label_map_util\n",
        "    from object_detection.utils import visualization_utils as viz_utils\n",
        "    #  ekhane directory te change ashbe from now on\n",
        "    PATH_TO_SAVED_MODEL = \"/content/gdrive/MyDrive/customTF2/data/inference_graph (1)/saved_model\"\n",
        "\n",
        "    print('Loading model...', end='')\n",
        "    start_time = time.time()\n",
        "\n",
        "    # LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\n",
        "    detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print('Done! Took {} seconds'.format(elapsed_time))\n",
        "\n",
        "    # LOAD LABEL MAP DATA FOR PLOTTING\n",
        "\n",
        "    category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
        "                                                                        use_display_name=True)\n",
        "\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "    import matplotlib.pyplot as plt\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "\n",
        "    def load_image_into_numpy_array(path):\n",
        "        \"\"\"Load an image from file into a numpy array.\n",
        "        Puts image into numpy array to feed into tensorflow graph.\n",
        "        Note that by convention we put it into a numpy array with shape\n",
        "        (height, width, channels), where channels=3 for RGB.\n",
        "        Args:\n",
        "          path: the file path to the image\n",
        "        Returns:\n",
        "          uint8 numpy array with shape (img_height, img_width, 3)\n",
        "        \"\"\"\n",
        "        return np.array(Image.open(path))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('Running inference for {}... '.format(IMAGE_PATHS), end='')\n",
        "\n",
        "    image = cv2.imread(IMAGE_PATHS)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_expanded = np.expand_dims(image_rgb, axis=0)\n",
        "\n",
        "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "    input_tensor = tf.convert_to_tensor(image)\n",
        "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "    input_tensor = input_tensor[tf.newaxis, ...]\n",
        "\n",
        "    # input_tensor = np.expand_dims(image_np, 0)\n",
        "    detections = detect_fn(input_tensor)\n",
        "    temp__num_detection = 0\n",
        "    # print(detections['detection_scores'][0])\n",
        "    for i in range(0,100):\n",
        "      # print(detections['detection_scores'][0][i])\n",
        "      if detections['detection_scores'][0][i]>0.2 :\n",
        "        # print(detections['detection_classes'][i])\n",
        "        # print(detections['detection_scores'][i])\n",
        "        temp__num_detection+=1\n",
        "    print(temp__num_detection)\n",
        "\n",
        "    detections['num_detections'] = temp__num_detection\n",
        "\n",
        "    # print ('\\n 1111111111111111111111111111111111 ')\n",
        "    # print(detections)\n",
        "\n",
        "    # All outputs are batches tensors.\n",
        "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "    # We're only interested in the first num_detections.\n",
        "    num_detections = int(detections.pop('num_detections'))\n",
        "\n",
        "\n",
        "    detections = {key: value[0, :num_detections].numpy()\n",
        "                  for key, value in detections.items()}\n",
        "    detections['num_detections'] = num_detections\n",
        "    print('hoye ja')\n",
        "    print(detections['detection_classes'])\n",
        "\n",
        "    # detection_classes should be ints.\n",
        "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "    image_with_detections = image.copy()\n",
        "\n",
        "    # SET MIN_SCORE_THRESH BASED ON YOU MINIMUM THRESHOLD FOR DETECTIONS\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "          image_with_detections,\n",
        "          detections['detection_boxes'],\n",
        "          detections['detection_classes'],\n",
        "          detections['detection_scores'],\n",
        "          category_index,\n",
        "          use_normalized_coordinates=True,\n",
        "          max_boxes_to_draw=100,\n",
        "          min_score_thresh=0.2,\n",
        "          agnostic_mode=False)\n",
        "\n",
        "    print('Done')\n",
        "    print(num_detections)\n",
        "\n",
        "    # for i in range(0,len(detections['detection_classes'])):\n",
        "    #   if detections['detection_scores'][i]>0.4 :\n",
        "    #     print(detections['detection_classes'][i])\n",
        "    #     print(detections['detection_scores'][i])\n",
        "    #     temp__num_detection+=1\n",
        "    # print(temp__num_detection)\n",
        "\n",
        "    print(detections['detection_classes'])\n",
        "    print(detections['detection_scores'])\n",
        "    # # DISPLAYS OUTPUT IMAGE\n",
        "    cv2_imshow(image_with_detections)\n",
        "    # CLOSES WINDOW ONCE KEY IS PRESSED\n",
        "\n",
        "    mark = [0]*num_detections\n",
        "    myletters = []\n",
        "    for i in range(0,num_detections):\n",
        "        cur=detections['detection_classes'][i]\n",
        "        current_label = labels[cur]\n",
        "        # ekhane change\n",
        "        if(current_label == 86):\n",
        "          current_label = 64\n",
        "        print('CURRENT_LABEL')\n",
        "        print(current_label)\n",
        "        print(classes[cur],end='-')\n",
        "        cur=detections['detection_scores'][i]\n",
        "\n",
        "        print(cur,end=' ')\n",
        "        print(detections['detection_boxes'][i], end=' ')\n",
        "        x0=(detections['detection_boxes'][i][0]) #xmin\n",
        "        y0=(detections['detection_boxes'][i][1]) #ymin\n",
        "        x1=(detections['detection_boxes'][i][2]) #xmax\n",
        "        y1=(detections['detection_boxes'][i][3]) #ymax\n",
        "        print('x0,y0,x1,y1')\n",
        "        print(x0,y0,x1,y1)\n",
        "        curarea=(x1-x0)*(y1-y0)\n",
        "        print('curarea')\n",
        "        print(curarea)\n",
        "        ok=1\n",
        "        for j in range(0,i):\n",
        "            #print(mark[j])\n",
        "            if mark[j]==0:\n",
        "                continue\n",
        "            x2=(detections['detection_boxes'][j][0]) #xmin\n",
        "            y2=(detections['detection_boxes'][j][1]) #ymin\n",
        "            x3=(detections['detection_boxes'][j][2]) #xmax\n",
        "            y3=(detections['detection_boxes'][j][3]) #ymax\n",
        "            x4=max(x0,x2) #larger value from two xmin\n",
        "            y4=max(y0,y2) #larger value from two ymin\n",
        "            x5=min(x1,x3) #smaller value from two xmax\n",
        "            y5=min(y1,y3) #smaller value from two ymax\n",
        "            print('x2,y2,x3,y3,x4,y4,x5,y5')\n",
        "            print(x2,y2,x3,y3,x4,y4,x5,y5)\n",
        "            previous_label = labels[detections['detection_classes'][j]]\n",
        "            # ekhane change\n",
        "            if(previous_label == 86):\n",
        "              previous_label = 64\n",
        "            print('PREVIOUS_LABEL')\n",
        "            print(previous_label)\n",
        "\n",
        "            if x4>x5 or y4>y5:  #both xmin must be less than both xmax and ymin <= ymax , otherwise no overlap\n",
        "                continue\n",
        "            print(\"EKHANE ASHCHI\") # we are here means overlapping case\n",
        "            prevarea=(x3-x2)*(y3-y2)\n",
        "            print('prevarea')\n",
        "            print(prevarea)\n",
        "            commonarea=(x5-x4)*(y5-y4)\n",
        "            print('commonarea')\n",
        "            print(commonarea)\n",
        "            ins1=curarea/commonarea # large common area means smaller ins\n",
        "            ins2=prevarea/commonarea\n",
        "            print('ins1,ins2')\n",
        "            print(ins1,ins2)\n",
        "            # print(ins1,end=' ')\n",
        "            # previous_label == 60 baad disi ekhan theke\n",
        "\n",
        "            characters_Eligible_For_Vowels =[11,12,13,14,16,17,18,19,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,49,68,69,70,71,72,73,74,75,76,77,78,79,81,82,83,84,85]\n",
        "            vowels = [61,62,91]\n",
        "            probable_clash_with_chondrobindu = [21,22,82]\n",
        "            # numbers_labels = [50,51,52,53,54,55,56,57,58,59]\n",
        "            if (((current_label in characters_Eligible_For_Vowels and previous_label in vowels) or (current_label in vowels and previous_label in characters_Eligible_For_Vowels) or\n",
        "             (current_label in probable_clash_with_chondrobindu and previous_label == 49) or (current_label == 49 and previous_label in probable_clash_with_chondrobindu)) and\n",
        "             (current_label != previous_label)):\n",
        "                if(ins1<0.5 or ins2<0.5):\n",
        "                    ok=0\n",
        "                    cur=detections['detection_classes'][j]\n",
        "                    print('classes[cur]')\n",
        "                    print(classes[cur])\n",
        "                    break\n",
        "            else:\n",
        "              print(\"TUKI3\")\n",
        "              if(ins1<2 or ins2<2): #\n",
        "                    ok=0\n",
        "                    cur=detections['detection_classes'][j]\n",
        "                    print('classes[cur]')\n",
        "                    print(classes[cur])\n",
        "                    break\n",
        "        if ok==1:\n",
        "            mark[i]=1\n",
        "            cur=detections['detection_classes'][i]\n",
        "            print(\"detections['detection_classes'][i]\")\n",
        "            print(cur)\n",
        "            #myletters.append(classes[cur])\n",
        "        print(ok)\n",
        "    for i in range(0,num_detections):\n",
        "        if mark[i]==0:\n",
        "            continue\n",
        "        cur=detections['detection_classes'][i]\n",
        "        cur=classes[cur]\n",
        "        y0=(detections['detection_boxes'][i][1])\n",
        "        pair = (y0,cur)\n",
        "        myletters.append(pair)\n",
        "    myletters.sort(key = lambda x: x[0])\n",
        "    #print(myletters)\n",
        "    res_list = [x[1] for x in myletters]\n",
        "    print(res_list)\n",
        "\n",
        "    #  output lekha\n",
        "    # final_Lekha = []\n",
        "\n",
        "    for i in range(len(res_list)-2, -1, -1):\n",
        "        x=res_list[i]\n",
        "        # print (\"x\",x)\n",
        "        y=res_list[i+1]\n",
        "        # print(\"y\",y)\n",
        "        if x=='ো':\n",
        "          res_list.pop(i)\n",
        "        if x=='ে' or x=='ি' or x=='ৈ' or x=='ৌ'or x=='্র' :\n",
        "            res_list[i],res_list[i+1]=res_list[i+1],res_list[i]\n",
        "    for i in range(len(res_list)-2, -1, -1):\n",
        "        x=res_list[i]\n",
        "        y=res_list[i+1]\n",
        "        # print(x,y)\n",
        "        if x=='অ' and y=='া':\n",
        "            print('yo')\n",
        "            res_list[i]='আ'\n",
        "            res_list.pop(i+1)\n",
        "    print(res_list)\n",
        "    for i in res_list:\n",
        "        print(i,end='')\n",
        "        final_Lekha.append(i)\n",
        "    final_Lekha.append(\" \")\n",
        "    # print (\"FINAL LEKHA\",final_Lekha)\n",
        "\n",
        "    # output lekha\n",
        "\n",
        "\n",
        "    label_id_offset = 1\n",
        "    image_np_with_detections = image_np.copy()\n",
        "    row,col,dummy=image_np.shape\n",
        "    list_with_all_boxes = []\n",
        "    if(isRealTime):\n",
        "        cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
        "    else:\n",
        "        plt.figure(num=None, figsize=(20,20), dpi=40, facecolor='w', edgecolor='k')\n",
        "        plt.imshow(image_np_with_detections, interpolation='nearest')\n",
        "        plt.gca().add_patch(Rectangle((10,60),10,10,edgecolor='r',facecolor='None'))\n",
        "        cntr=0\n",
        "        for i in range(0,num_detections):\n",
        "            if mark[i]==0:\n",
        "                continue\n",
        "            classname=detections['detection_classes'][i]\n",
        "            classname= labels[classname]\n",
        "            x0=(detections['detection_boxes'][i][0])*row\n",
        "            y0=(detections['detection_boxes'][i][1])*col\n",
        "            x1=(detections['detection_boxes'][i][2])*row\n",
        "            y1=(detections['detection_boxes'][i][3])*col\n",
        "            if cntr==0:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='r',facecolor='None'))\n",
        "                cntr=1\n",
        "            elif cntr==1:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='b',facecolor='None'))\n",
        "                cntr=2\n",
        "            elif cntr==2:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='k',facecolor='None'))\n",
        "                cntr=3\n",
        "            elif cntr==3:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='g',facecolor='None'))\n",
        "                cntr=0\n",
        "            list_with_single_boxes = [x0,y0,x1,y1,classname]\n",
        "            list_with_all_boxes.append(list_with_single_boxes)\n",
        "\n",
        "\n",
        "        #plt.xticks([])\n",
        "        #plt.yticks([])\n",
        "        #plt.show()\n",
        "    return list_with_all_boxes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WARNING!! Caution : Don't run this. Run this only if you want to check the result without the (8) PNO steps"
      ],
      "metadata": {
        "id": "aPWxvJV5MbpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Object Detection (On Image) From TF2 Saved Model\n",
        "=====================================\n",
        "\"\"\"\n",
        "final_Lekha = []\n",
        "def detect_frame(frame,IMAGE_PATHS,isRealTime = False):\n",
        "    # def detect_frame(frame,isRealTime = False):\n",
        "    import os\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
        "    import pathlib\n",
        "    import tensorflow as tf\n",
        "    import cv2\n",
        "    import argparse\n",
        "    import numpy as np\n",
        "    from google.colab.patches import cv2_imshow\n",
        "\n",
        "    # Enable GPU dynamic memory allocation\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "    # PROVIDE PATH TO IMAGE DIRECTORY\n",
        "    print(IMAGE_PATHS)\n",
        "    IMAGE_PATHS = f'{IMAGE_PATHS}'\n",
        "    # frame = cv2.imread((\"/content/gdrive/MyDrive/customTF2/data/images/0.bmp\"))\n",
        "\n",
        "    # IMAGE_PATHS= frame\n",
        "    image_np = np.array(frame)\n",
        "    # PROVIDE PATH TO MODEL DIRECTORY\n",
        "    PATH_TO_MODEL_DIR = '/content/gdrive/MyDrive/customTF2/data/inference_graph (1)'\n",
        "\n",
        "    # PROVIDE PATH TO LABEL MAP\n",
        "    PATH_TO_LABELS = '/content/gdrive/MyDrive/customTF2/data/label_map.pbtxt'\n",
        "\n",
        "    # PROVIDE THE MINIMUM CONFIDENCE THRESHOLD\n",
        "    MIN_CONF_THRESH = float(0.20)\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "\n",
        "    import time\n",
        "    from object_detection.utils import label_map_util\n",
        "    from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "    PATH_TO_SAVED_MODEL = \"/content/gdrive/MyDrive/customTF2/data/inference_graph (1)/saved_model\"\n",
        "\n",
        "    print('Loading model...', end='')\n",
        "    start_time = time.time()\n",
        "\n",
        "    # LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\n",
        "    detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print('Done! Took {} seconds'.format(elapsed_time))\n",
        "\n",
        "    # LOAD LABEL MAP DATA FOR PLOTTING\n",
        "\n",
        "    category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
        "                                                                        use_display_name=True)\n",
        "\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "    import matplotlib.pyplot as plt\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "\n",
        "    def load_image_into_numpy_array(path):\n",
        "        \"\"\"Load an image from file into a numpy array.\n",
        "        Puts image into numpy array to feed into tensorflow graph.\n",
        "        Note that by convention we put it into a numpy array with shape\n",
        "        (height, width, channels), where channels=3 for RGB.\n",
        "        Args:\n",
        "          path: the file path to the image\n",
        "        Returns:\n",
        "          uint8 numpy array with shape (img_height, img_width, 3)\n",
        "        \"\"\"\n",
        "        return np.array(Image.open(path))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('Running inference for {}... '.format(IMAGE_PATHS), end='')\n",
        "\n",
        "    image = cv2.imread(IMAGE_PATHS)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_expanded = np.expand_dims(image_rgb, axis=0)\n",
        "\n",
        "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "    input_tensor = tf.convert_to_tensor(image)\n",
        "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "    input_tensor = input_tensor[tf.newaxis, ...]\n",
        "\n",
        "    # input_tensor = np.expand_dims(image_np, 0)\n",
        "    detections = detect_fn(input_tensor)\n",
        "    temp__num_detection = 0\n",
        "    # print(detections['detection_scores'][0])\n",
        "    for i in range(0,100):\n",
        "      # print(detections['detection_scores'][0][i])\n",
        "      if detections['detection_scores'][0][i]>0.2 :\n",
        "        # print(detections['detection_classes'][i])\n",
        "        # print(detections['detection_scores'][i])\n",
        "        temp__num_detection+=1\n",
        "    print(temp__num_detection)\n",
        "\n",
        "    detections['num_detections'] = temp__num_detection\n",
        "\n",
        "    # print ('\\n 1111111111111111111111111111111111 ')\n",
        "    # print(detections)\n",
        "\n",
        "    # All outputs are batches tensors.\n",
        "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "    # We're only interested in the first num_detections.\n",
        "    num_detections = int(detections.pop('num_detections'))\n",
        "\n",
        "\n",
        "    detections = {key: value[0, :num_detections].numpy()\n",
        "                  for key, value in detections.items()}\n",
        "    detections['num_detections'] = num_detections\n",
        "    print('hoye ja')\n",
        "    print(detections['detection_classes'])\n",
        "\n",
        "    # detection_classes should be ints.\n",
        "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "    image_with_detections = image.copy()\n",
        "\n",
        "    # SET MIN_SCORE_THRESH BASED ON YOU MINIMUM THRESHOLD FOR DETECTIONS\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "          image_with_detections,\n",
        "          detections['detection_boxes'],\n",
        "          detections['detection_classes'],\n",
        "          detections['detection_scores'],\n",
        "          category_index,\n",
        "          use_normalized_coordinates=True,\n",
        "          max_boxes_to_draw=100,\n",
        "          min_score_thresh=0.2,\n",
        "          agnostic_mode=False)\n",
        "\n",
        "    print('Done')\n",
        "    print(num_detections)\n",
        "\n",
        "    # for i in range(0,len(detections['detection_classes'])):\n",
        "    #   if detections['detection_scores'][i]>0.4 :\n",
        "    #     print(detections['detection_classes'][i])\n",
        "    #     print(detections['detection_scores'][i])\n",
        "    #     temp__num_detection+=1\n",
        "    # print(temp__num_detection)\n",
        "\n",
        "    print(detections['detection_classes'])\n",
        "    print(detections['detection_scores'])\n",
        "    # # DISPLAYS OUTPUT IMAGE\n",
        "    cv2_imshow(image_with_detections)\n",
        "    # CLOSES WINDOW ONCE KEY IS PRESSED\n",
        "\n",
        "    mark = [0]*num_detections\n",
        "    myletters = []\n",
        "    for i in range(0,num_detections):\n",
        "        cur=detections['detection_classes'][i]\n",
        "        current_label = labels[cur]\n",
        "        # ekhane change\n",
        "        if(current_label == 86):\n",
        "          current_label = 64\n",
        "        print('CURRENT_LABEL')\n",
        "        print(current_label)\n",
        "        print(classes[cur],end='-')\n",
        "        cur=detections['detection_scores'][i]\n",
        "\n",
        "        print(cur,end=' ')\n",
        "        print(detections['detection_boxes'][i], end=' ')\n",
        "        x0=(detections['detection_boxes'][i][0]) #xmin\n",
        "        y0=(detections['detection_boxes'][i][1]) #ymin\n",
        "        x1=(detections['detection_boxes'][i][2]) #xmax\n",
        "        y1=(detections['detection_boxes'][i][3]) #ymax\n",
        "        print('x0,y0,x1,y1')\n",
        "        print(x0,y0,x1,y1)\n",
        "        curarea=(x1-x0)*(y1-y0)\n",
        "        print('curarea')\n",
        "        print(curarea)\n",
        "        ok=1\n",
        "        for j in range(0,i):\n",
        "            #print(mark[j])\n",
        "            if mark[j]==0:\n",
        "                continue\n",
        "            x2=(detections['detection_boxes'][j][0]) #xmin\n",
        "            y2=(detections['detection_boxes'][j][1]) #ymin\n",
        "            x3=(detections['detection_boxes'][j][2]) #xmax\n",
        "            y3=(detections['detection_boxes'][j][3]) #ymax\n",
        "            x4=max(x0,x2) #larger value from two xmin\n",
        "            y4=max(y0,y2) #larger value from two ymin\n",
        "            x5=min(x1,x3) #smaller value from two xmax\n",
        "            y5=min(y1,y3) #smaller value from two ymax\n",
        "            print('x2,y2,x3,y3,x4,y4,x5,y5')\n",
        "            print(x2,y2,x3,y3,x4,y4,x5,y5)\n",
        "            previous_label = labels[detections['detection_classes'][j]]\n",
        "            # ekhane change\n",
        "            if(previous_label == 86):\n",
        "              previous_label = 64\n",
        "            print('PREVIOUS_LABEL')\n",
        "            print(previous_label)\n",
        "\n",
        "            if x4>x5 or y4>y5:  #both xmin must be less than both xmax and ymin <= ymax , otherwise no overlap\n",
        "                continue\n",
        "            print(\"EKHANE ASHCHI\") # we are here means overlapping case\n",
        "            prevarea=(x3-x2)*(y3-y2)\n",
        "            print('prevarea')\n",
        "            print(prevarea)\n",
        "            commonarea=(x5-x4)*(y5-y4)\n",
        "            print('commonarea')\n",
        "            print(commonarea)\n",
        "            ins1=curarea/commonarea # large common area means smaller ins\n",
        "            ins2=prevarea/commonarea\n",
        "            print('ins1,ins2')\n",
        "            print(ins1,ins2)\n",
        "            # print(ins1,end=' ')\n",
        "            # previous_label == 60 baad disi ekhan theke\n",
        "            #  ঁ,ি ,ী, ে ,ূ ,ু ,্য ,ৃ ,ৌ ,ৈ, । eshob konotay thakle beshi common eo nibe\n",
        "\n",
        "            print(\"TUKI3\")\n",
        "            if(ins1<2 or ins2<2): #\n",
        "                  ok=0\n",
        "                  cur=detections['detection_classes'][j]\n",
        "                  print('classes[cur]')\n",
        "                  print(classes[cur])\n",
        "                  break\n",
        "        if ok==1:\n",
        "            mark[i]=1\n",
        "            cur=detections['detection_classes'][i]\n",
        "            print(\"detections['detection_classes'][i]\")\n",
        "            print(cur)\n",
        "            #myletters.append(classes[cur])\n",
        "        print(ok)\n",
        "    for i in range(0,num_detections):\n",
        "        if mark[i]==0:\n",
        "            continue\n",
        "        cur=detections['detection_classes'][i]\n",
        "        cur=classes[cur]\n",
        "        y0=(detections['detection_boxes'][i][1])\n",
        "        pair = (y0,cur)\n",
        "        myletters.append(pair)\n",
        "    myletters.sort(key = lambda x: x[0])\n",
        "    #print(myletters)\n",
        "    res_list = [x[1] for x in myletters]\n",
        "    print(res_list)\n",
        "\n",
        "    #  output lekha\n",
        "    # final_Lekha = []\n",
        "\n",
        "    for i in range(len(res_list)-2, -1, -1):\n",
        "        x=res_list[i]\n",
        "        # print (\"x\",x)\n",
        "        y=res_list[i+1]\n",
        "        # print(\"y\",y)\n",
        "        if x=='ো':\n",
        "          res_list.pop(i)\n",
        "        if x=='ে' or x=='ি' or x=='ৈ' or x=='ৌ':\n",
        "            res_list[i],res_list[i+1]=res_list[i+1],res_list[i]\n",
        "    for i in range(len(res_list)-2, -1, -1):\n",
        "        x=res_list[i]\n",
        "        y=res_list[i+1]\n",
        "        # print(x,y)\n",
        "        if x=='অ' and y=='া':\n",
        "            print('yo')\n",
        "            res_list[i]='আ'\n",
        "            res_list.pop(i+1)\n",
        "    print(res_list)\n",
        "    for i in res_list:\n",
        "        print(i,end='')\n",
        "        final_Lekha.append(i)\n",
        "    final_Lekha.append(\" \")\n",
        "    # print (\"FINAL LEKHA\",final_Lekha)\n",
        "\n",
        "    # output lekha\n",
        "\n",
        "\n",
        "    label_id_offset = 1\n",
        "    image_np_with_detections = image_np.copy()\n",
        "    row,col,dummy=image_np.shape\n",
        "    list_with_all_boxes = []\n",
        "    if(isRealTime):\n",
        "        cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
        "    else:\n",
        "        plt.figure(num=None, figsize=(20,20), dpi=40, facecolor='w', edgecolor='k')\n",
        "        plt.imshow(image_np_with_detections, interpolation='nearest')\n",
        "        plt.gca().add_patch(Rectangle((10,60),10,10,edgecolor='r',facecolor='None'))\n",
        "        cntr=0\n",
        "        for i in range(0,num_detections):\n",
        "            if mark[i]==0:\n",
        "                continue\n",
        "            classname=detections['detection_classes'][i]\n",
        "            classname= labels[classname]\n",
        "            x0=(detections['detection_boxes'][i][0])*row\n",
        "            y0=(detections['detection_boxes'][i][1])*col\n",
        "            x1=(detections['detection_boxes'][i][2])*row\n",
        "            y1=(detections['detection_boxes'][i][3])*col\n",
        "            if cntr==0:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='r',facecolor='None'))\n",
        "                cntr=1\n",
        "            elif cntr==1:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='b',facecolor='None'))\n",
        "                cntr=2\n",
        "            elif cntr==2:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='k',facecolor='None'))\n",
        "                cntr=3\n",
        "            elif cntr==3:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='g',facecolor='None'))\n",
        "                cntr=0\n",
        "            list_with_single_boxes = [x0,y0,x1,y1,classname]\n",
        "            list_with_all_boxes.append(list_with_single_boxes)\n",
        "\n",
        "\n",
        "        #plt.xticks([])\n",
        "        #plt.yticks([])\n",
        "        #plt.show()\n",
        "    return list_with_all_boxes"
      ],
      "metadata": {
        "id": "qhfs_eLSMaMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "WARNING !! Caution : Don't run this. Run this only if you want to check the result without the (7)NMS and (8) PNO steps"
      ],
      "metadata": {
        "id": "HrvvCa-TMrAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Object Detection (On Image) From TF2 Saved Model\n",
        "=====================================\n",
        "\"\"\"\n",
        "final_Lekha = []\n",
        "def detect_frame(frame,IMAGE_PATHS,isRealTime = False):\n",
        "    # def detect_frame(frame,isRealTime = False):\n",
        "    import os\n",
        "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
        "    import pathlib\n",
        "    import tensorflow as tf\n",
        "    import cv2\n",
        "    import argparse\n",
        "    import numpy as np\n",
        "    from google.colab.patches import cv2_imshow\n",
        "\n",
        "    # Enable GPU dynamic memory allocation\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "    # PROVIDE PATH TO IMAGE DIRECTORY\n",
        "    print(IMAGE_PATHS)\n",
        "    IMAGE_PATHS = f'{IMAGE_PATHS}'\n",
        "    # frame = cv2.imread((\"/content/gdrive/MyDrive/customTF2/data/images/0.bmp\"))\n",
        "\n",
        "    # IMAGE_PATHS= frame\n",
        "    image_np = np.array(frame)\n",
        "    # PROVIDE PATH TO MODEL DIRECTORY\n",
        "    PATH_TO_MODEL_DIR = '/content/gdrive/MyDrive/customTF2/data/inference_graph (1)'\n",
        "\n",
        "    # PROVIDE PATH TO LABEL MAP\n",
        "    PATH_TO_LABELS = '/content/gdrive/MyDrive/customTF2/data/label_map.pbtxt'\n",
        "\n",
        "    # PROVIDE THE MINIMUM CONFIDENCE THRESHOLD\n",
        "    MIN_CONF_THRESH = float(0.20)\n",
        "\n",
        "    # LOAD THE MODEL\n",
        "\n",
        "    import time\n",
        "    from object_detection.utils import label_map_util\n",
        "    from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "    PATH_TO_SAVED_MODEL = \"/content/gdrive/MyDrive/customTF2/data/inference_graph (1)/saved_model\"\n",
        "\n",
        "    print('Loading model...', end='')\n",
        "    start_time = time.time()\n",
        "\n",
        "    # LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\n",
        "    detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print('Done! Took {} seconds'.format(elapsed_time))\n",
        "\n",
        "    # LOAD LABEL MAP DATA FOR PLOTTING\n",
        "\n",
        "    category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
        "                                                                        use_display_name=True)\n",
        "\n",
        "    import numpy as np\n",
        "    from PIL import Image\n",
        "    import matplotlib.pyplot as plt\n",
        "    import warnings\n",
        "    warnings.filterwarnings('ignore')   # Suppress Matplotlib warnings\n",
        "\n",
        "    def load_image_into_numpy_array(path):\n",
        "        \"\"\"Load an image from file into a numpy array.\n",
        "        Puts image into numpy array to feed into tensorflow graph.\n",
        "        Note that by convention we put it into a numpy array with shape\n",
        "        (height, width, channels), where channels=3 for RGB.\n",
        "        Args:\n",
        "          path: the file path to the image\n",
        "        Returns:\n",
        "          uint8 numpy array with shape (img_height, img_width, 3)\n",
        "        \"\"\"\n",
        "        return np.array(Image.open(path))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print('Running inference for {}... '.format(IMAGE_PATHS), end='')\n",
        "\n",
        "    image = cv2.imread(IMAGE_PATHS)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image_expanded = np.expand_dims(image_rgb, axis=0)\n",
        "\n",
        "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "    input_tensor = tf.convert_to_tensor(image)\n",
        "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "    input_tensor = input_tensor[tf.newaxis, ...]\n",
        "\n",
        "    # input_tensor = np.expand_dims(image_np, 0)\n",
        "    detections = detect_fn(input_tensor)\n",
        "    temp__num_detection = 0\n",
        "    # print(detections['detection_scores'][0])\n",
        "    for i in range(0,100):\n",
        "      # print(detections['detection_scores'][0][i])\n",
        "      if detections['detection_scores'][0][i]>0.2 :\n",
        "        # print(detections['detection_classes'][i])\n",
        "        # print(detections['detection_scores'][i])\n",
        "        temp__num_detection+=1\n",
        "    print(temp__num_detection)\n",
        "\n",
        "    detections['num_detections'] = temp__num_detection\n",
        "\n",
        "    # print ('\\n 1111111111111111111111111111111111 ')\n",
        "    # print(detections)\n",
        "\n",
        "    # All outputs are batches tensors.\n",
        "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "    # We're only interested in the first num_detections.\n",
        "    num_detections = int(detections.pop('num_detections'))\n",
        "\n",
        "\n",
        "    detections = {key: value[0, :num_detections].numpy()\n",
        "                  for key, value in detections.items()}\n",
        "    detections['num_detections'] = num_detections\n",
        "    print('hoye ja')\n",
        "    print(detections['detection_classes'])\n",
        "\n",
        "    # detection_classes should be ints.\n",
        "    detections['detection_classes'] = detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "    image_with_detections = image.copy()\n",
        "\n",
        "    # SET MIN_SCORE_THRESH BASED ON YOU MINIMUM THRESHOLD FOR DETECTIONS\n",
        "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
        "          image_with_detections,\n",
        "          detections['detection_boxes'],\n",
        "          detections['detection_classes'],\n",
        "          detections['detection_scores'],\n",
        "          category_index,\n",
        "          use_normalized_coordinates=True,\n",
        "          max_boxes_to_draw=100,\n",
        "          min_score_thresh=0.2,\n",
        "          agnostic_mode=False)\n",
        "\n",
        "    print('Done')\n",
        "    print(num_detections)\n",
        "\n",
        "    # for i in range(0,len(detections['detection_classes'])):\n",
        "    #   if detections['detection_scores'][i]>0.4 :\n",
        "    #     print(detections['detection_classes'][i])\n",
        "    #     print(detections['detection_scores'][i])\n",
        "    #     temp__num_detection+=1\n",
        "    # print(temp__num_detection)\n",
        "\n",
        "    print(detections['detection_classes'])\n",
        "    print(detections['detection_scores'])\n",
        "    # # DISPLAYS OUTPUT IMAGE\n",
        "    cv2_imshow(image_with_detections)\n",
        "    # CLOSES WINDOW ONCE KEY IS PRESSED\n",
        "\n",
        "    mark = [0]*num_detections\n",
        "    myletters = []\n",
        "    for i in range(0,num_detections):\n",
        "        cur=detections['detection_classes'][i]\n",
        "        current_label = labels[cur]\n",
        "        # ekhane change\n",
        "        if(current_label == 86):\n",
        "          current_label = 64\n",
        "        print('CURRENT_LABEL')\n",
        "        print(current_label)\n",
        "        print(classes[cur],end='-')\n",
        "        cur=detections['detection_scores'][i]\n",
        "\n",
        "        print(cur,end=' ')\n",
        "        print(detections['detection_boxes'][i], end=' ')\n",
        "\n",
        "        mark[i]=1\n",
        "        cur=detections['detection_classes'][i]\n",
        "        print(\"detections['detection_classes'][i]\")\n",
        "        print(cur)\n",
        "        #myletters.append(classes[cur])\n",
        "\n",
        "    for i in range(0,num_detections):\n",
        "        if mark[i]==0:\n",
        "            continue\n",
        "        cur=detections['detection_classes'][i]\n",
        "        cur=classes[cur]\n",
        "        y0=(detections['detection_boxes'][i][1])\n",
        "        pair = (y0,cur)\n",
        "        myletters.append(pair)\n",
        "    myletters.sort(key = lambda x: x[0])\n",
        "    #print(myletters)\n",
        "    res_list = [x[1] for x in myletters]\n",
        "    print(res_list)\n",
        "\n",
        "    #  output lekha\n",
        "    # final_Lekha = []\n",
        "\n",
        "    for i in range(len(res_list)-2, -1, -1):\n",
        "        x=res_list[i]\n",
        "        # print (\"x\",x)\n",
        "        y=res_list[i+1]\n",
        "        # print(\"y\",y)\n",
        "        if x=='ো':\n",
        "          res_list.pop(i)\n",
        "        if x=='ে' or x=='ি' or x=='ৈ' or x=='ৌ':\n",
        "            res_list[i],res_list[i+1]=res_list[i+1],res_list[i]\n",
        "    for i in range(len(res_list)-2, -1, -1):\n",
        "        x=res_list[i]\n",
        "        y=res_list[i+1]\n",
        "        # print(x,y)\n",
        "        if x=='অ' and y=='া':\n",
        "            print('yo')\n",
        "            res_list[i]='আ'\n",
        "            res_list.pop(i+1)\n",
        "    print(res_list)\n",
        "    for i in res_list:\n",
        "        print(i,end='')\n",
        "        final_Lekha.append(i)\n",
        "    final_Lekha.append(\" \")\n",
        "    # print (\"FINAL LEKHA\",final_Lekha)\n",
        "\n",
        "    # output lekha\n",
        "\n",
        "\n",
        "    label_id_offset = 1\n",
        "    image_np_with_detections = image_np.copy()\n",
        "    row,col,dummy=image_np.shape\n",
        "    list_with_all_boxes = []\n",
        "    if(isRealTime):\n",
        "        cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))\n",
        "    else:\n",
        "        plt.figure(num=None, figsize=(20,20), dpi=40, facecolor='w', edgecolor='k')\n",
        "        plt.imshow(image_np_with_detections, interpolation='nearest')\n",
        "        plt.gca().add_patch(Rectangle((10,60),10,10,edgecolor='r',facecolor='None'))\n",
        "        cntr=0\n",
        "        for i in range(0,num_detections):\n",
        "            if mark[i]==0:\n",
        "                continue\n",
        "            classname=detections['detection_classes'][i]\n",
        "            classname= labels[classname]\n",
        "            x0=(detections['detection_boxes'][i][0])*row\n",
        "            y0=(detections['detection_boxes'][i][1])*col\n",
        "            x1=(detections['detection_boxes'][i][2])*row\n",
        "            y1=(detections['detection_boxes'][i][3])*col\n",
        "            if cntr==0:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='r',facecolor='None'))\n",
        "                cntr=1\n",
        "            elif cntr==1:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='b',facecolor='None'))\n",
        "                cntr=2\n",
        "            elif cntr==2:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='k',facecolor='None'))\n",
        "                cntr=3\n",
        "            elif cntr==3:\n",
        "                plt.gca().add_patch(Rectangle((y0,x0),y1-y0,x1-x0,linewidth=3,edgecolor='g',facecolor='None'))\n",
        "                cntr=0\n",
        "            list_with_single_boxes = [x0,y0,x1,y1,classname]\n",
        "            list_with_all_boxes.append(list_with_single_boxes)\n",
        "\n",
        "\n",
        "        #plt.xticks([])\n",
        "        #plt.yticks([])\n",
        "        #plt.show()\n",
        "    return list_with_all_boxes"
      ],
      "metadata": {
        "id": "H0aHkiKbMndV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSIy6KJHZEeZ"
      },
      "outputs": [],
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "\n",
        "def read_content(xml_file: str):\n",
        "\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    list_with_all_boxes = []\n",
        "\n",
        "    for boxes in root.iter('object'):\n",
        "\n",
        "        filename = root.find('filename').text\n",
        "        classname = None\n",
        "        ymin, xmin, ymax, xmax = None, None, None, None\n",
        "        classname = int(boxes.find(\"name\").text)\n",
        "        ymin = int(boxes.find(\"bndbox/ymin\").text)\n",
        "        xmin = int(boxes.find(\"bndbox/xmin\").text)\n",
        "        ymax = int(boxes.find(\"bndbox/ymax\").text)\n",
        "        xmax = int(boxes.find(\"bndbox/xmax\").text)\n",
        "\n",
        "        list_with_single_boxes = [xmin, ymin, xmax, ymax,classname]\n",
        "        list_with_all_boxes.append(list_with_single_boxes)\n",
        "\n",
        "    return list_with_all_boxes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57brPaUHZHGv",
        "outputId": "f2ddd39e-3838-4cca-caca-7d3b6d37c9fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path(r\"/content/gdrive/MyDrive/customTF2/data/Test\")\n",
        "# PATH_TO_TEST_IMAGES_DIR = pathlib.Path('testImagesWithLabel')\n",
        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.bmp\"))) #+list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.png\"))\n",
        "TEST_XML_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.xml\")))\n",
        "print(len(TEST_IMAGE_PATHS))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# len(TEST_IMAGE_PATHS)\n",
        "TP = [0]*93\n",
        "FP = [0]*93\n",
        "FN = [0]*93\n",
        "print(FP)\n",
        "\n",
        "TP_OLD = [0]*93\n",
        "FP_OLD = [0]*93\n",
        "FN_OLD = [0]*93"
      ],
      "metadata": {
        "id": "jRech_JCNbBa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(TEST_IMAGE_PATHS)\n",
        "# TP = [0]*93\n",
        "# FP = [0]*93\n",
        "# FN = [0]*93\n",
        "# print(FP)\n",
        "\n",
        "# TP_OLD = [0]*93\n",
        "# FP_OLD = [0]*93\n",
        "# FN_OLD = [0]*93\n",
        "confusion_matrix = [[0 for i in range(93)] for j in range(93)]\n",
        "confusion_matrix_OLD = [[0 for i in range(93)] for j in range(93)]\n",
        "\n",
        "\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "# 1280\n",
        "for k in range(0,len(TEST_IMAGE_PATHS)):\n",
        "    print(\"K NUMBER = \",k)\n",
        "    image_path = TEST_IMAGE_PATHS[k]\n",
        "    print(image_path)\n",
        "    xml_path = TEST_XML_PATHS[k]\n",
        "    print(xml_path)\n",
        "    frame = cv2.imread(str(image_path))\n",
        "    # gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    cv2_imshow(frame)\n",
        "    A = detect_frame(frame,image_path)\n",
        "    B = read_content(xml_path)\n",
        "    print(\"A-----------------------------------------------------------\")\n",
        "\n",
        "\n",
        "    print(A)\n",
        "    print(B)\n",
        "\n",
        "    biou=0.5\n",
        "    markA = [0]*len(A)\n",
        "    mark=[0]*len(B)\n",
        "    maxiou=0.4\n",
        "    for j in range(0,len(B)):\n",
        "\n",
        "        x2=B[j][0]\n",
        "        y2=B[j][1]\n",
        "        x3=B[j][2]\n",
        "        y3=B[j][3]\n",
        "        classB=B[j][4]\n",
        "        if(classB == 86):\n",
        "          classB = 64\n",
        "        print(\"actual class\",classB)\n",
        "        OriginalArea=(x3-x2)*(y3-y2)\n",
        "        got = 0\n",
        "        for i in range(0,len(A)):\n",
        "            if markA[i]==1:\n",
        "              print(\"EKHANE\")\n",
        "              continue\n",
        "            y0=A[i][0]\n",
        "            x0=A[i][1]\n",
        "            y1=A[i][2]\n",
        "            x1=A[i][3]\n",
        "            classA=A[i][4]\n",
        "            if(classA == 86):\n",
        "              classA = 64\n",
        "            # print(\"predicted class\",classA)\n",
        "            x4=max(x0,x2)\n",
        "            y4=max(y0,y2)\n",
        "            x5=min(x1,x3)\n",
        "            y5=min(y1,y3)\n",
        "            ComputedArea=(x1-x0)*(y1-y0)\n",
        "            commonarea=(x5-x4)*(y5-y4)\n",
        "            iou=commonarea/(ComputedArea+OriginalArea-commonarea)\n",
        "            if x4>x5 or y4>y5:\n",
        "                continue\n",
        "            # print(\"IOU\",iou)\n",
        "            # print(\"MAXIOU\",maxiou)\n",
        "            if x4>x5 or y4>y5:\n",
        "                continue\n",
        "            if iou>maxiou:\n",
        "              got = 1\n",
        "              mxx=i\n",
        "              markA[mxx]=1\n",
        "              if (classA == classB):\n",
        "                TP[classA]=TP[classA]+1\n",
        "                confusion_matrix[classB][classA] = confusion_matrix[classB][classA] + 1\n",
        "                # print(\"TRUE Postive\",\"classA=\",classA)\n",
        "                break\n",
        "              else:\n",
        "                FP[classA]=FP[classA]+1\n",
        "                confusion_matrix[classB][classA] = confusion_matrix[classB][classA] + 1\n",
        "                # print(\"False Postive\",\"classA=\",classA,\"classB=\",classB)\n",
        "                break\n",
        "\n",
        "        if got==0:\n",
        "          FN[classB]=FN[classB]+1\n",
        "          confusion_matrix[classB][0] = confusion_matrix[classB][0] + 1\n",
        "          # print(\"False Negative\",\"classB=\",classB)\n",
        "    for i in range(0,len(A)):\n",
        "      if(markA[i]==0):\n",
        "        classA=A[i][4]\n",
        "        if(classA == 86):\n",
        "          classA = 64\n",
        "        FP[classA]=FP[classA]+1\n",
        "        confusion_matrix[0][classA] = confusion_matrix[0][classA]+ 1\n",
        "\n",
        "    for i in range (len(TP)):\n",
        "      if(TP[i]!=TP_OLD[i]):\n",
        "        print(\"TP CHANGE index = \",i,\" TP DIFFERENCE = \",[TP_OLD[i]-TP[i]])\n",
        "      if(FP[i]!=FP_OLD[i]):\n",
        "        print(\"FP CHANGE index = \",i,\" FP DIFFERENCE = \",[FP_OLD[i]-FP[i]])\n",
        "      if(FN[i]!=FN_OLD[i]):\n",
        "        print(\"FN CHANGE index = \",i,\" FN DIFFERENCE = \",[FN_OLD[i]-FN[i]])\n",
        "\n",
        "    for m in range(0,93):\n",
        "      for n in range(0,93):\n",
        "        if (confusion_matrix [m][n] != confusion_matrix_OLD[m][n]):\n",
        "          print (\"confusion_matrix_row = \",classes[m],\" COLUMN = \",classes[n],\" DIFFERENCE = \",confusion_matrix [m][n] - confusion_matrix_OLD[m][n])\n",
        "          confusion_matrix_OLD [m][n] = confusion_matrix[m][n]\n",
        "    # print(\"OIOIIIIOOII\",confusion_matrix[53][53], confusion_matrix_OLD[53][53])\n",
        "    TP_OLD = TP.copy()\n",
        "    FP_OLD = FP.copy()\n",
        "    FN_OLD = FN.copy()\n",
        "\n",
        "\n",
        "    print(\"TP = \",TP)\n",
        "    print(\"FP = \",FP)\n",
        "    print(\"FN = \",FN)\n",
        "    print(\"TP_OLD = \",TP)\n",
        "    print(\"FP_OLD = \",FP)\n",
        "    print(\"FN_OLD = \",FN)\n",
        "    # print(confusion_matrix)\n",
        "    print (\" \")"
      ],
      "metadata": {
        "id": "iUXeRgjwNqct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: In case Lenshtein Distance is to be checked"
      ],
      "metadata": {
        "id": "-laGMNSoKFlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import cmp_to_key\n",
        "correct_word_percentage = [0, 0, 0, 0, 0]\n",
        "from google.colab.patches import cv2_imshow\n",
        "# [0,0,0,0,0]\n",
        "def sortbyCond(a, b):\n",
        "    print (\"a = \", a)\n",
        "    print (\"b = \", b)\n",
        "\n",
        "    if (a[0] != b[0]):\n",
        "        print ( \"a[0] - b[0]\",a[0] - b[0])\n",
        "        return (a[0] - b[0])\n",
        "    else:\n",
        "        print (\"b[1] - a[1]\",b[1] - a[1])\n",
        "        return (b[1] - a[1])\n",
        "\n",
        "def levenshtein(a, b):\n",
        "    \"\"\"Calculates the Levenshtein distance between a and b.\"\"\"\n",
        "    n, m = len(a), len(b)\n",
        "    if n > m:\n",
        "        a, b = b, a\n",
        "        n, m = m, n\n",
        "    current = range(n + 1)\n",
        "    #print(current)\n",
        "    for i in range(1, m + 1):\n",
        "        previous, current = current, [i] + [0] * n\n",
        "        #print(previous)\n",
        "        for j in range(1, n + 1):\n",
        "            add, delete = previous[j] + 1, current[(j - 1)] + 1\n",
        "            change = previous[(j - 1)]\n",
        "            if a[(j - 1)] != b[(i - 1)]:\n",
        "                change = change + 1\n",
        "            current[j] = min(add, delete, change)\n",
        "\n",
        "    return current[n]\n",
        "for k in range(0,len(TEST_IMAGE_PATHS)):\n",
        "    print(\"K NUMBER = \",k)\n",
        "    image_path = TEST_IMAGE_PATHS[k]\n",
        "    print(image_path)\n",
        "    xml_path = TEST_XML_PATHS[k]\n",
        "    print(xml_path)\n",
        "    frame = cv2.imread(str(image_path))\n",
        "    # gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    cv2_imshow(frame)\n",
        "    A = detect_frame(frame,image_path)\n",
        "    B = read_content(xml_path)\n",
        "    # print(\"A-----------------------------------------------------------\")\n",
        "    # print(A)\n",
        "    # print(B)\n",
        "    arr1 = A\n",
        "    arr2 = B\n",
        "    sorted_a1 = []\n",
        "    sorted_a2 = []\n",
        "    # print (\"A = \",A)\n",
        "    # print (\"B = \",B)\n",
        "    for i in range (0,len(arr1)):\n",
        "      sorted_a1.append([arr1[i][1], arr1[i][2]])\n",
        "    # print (\"sorted_a1\",sorted_a1)\n",
        "    for i in range (0,len(arr2)):\n",
        "      sorted_a2.append([arr2[i][0], arr2[i][3]])\n",
        "    # print (\"sorted_a2\",sorted_a2)\n",
        "    sorted_a1.sort(key = cmp_to_key(sortbyCond))\n",
        "    sorted_a2.sort(key = cmp_to_key(sortbyCond))\n",
        "    #  x axis er value er upor base kore sort\n",
        "\n",
        "    C =[]\n",
        "    D=[]\n",
        "\n",
        "    for i in range(0,len(sorted_a1)):\n",
        "      x = sorted_a1[i][0]\n",
        "      y = sorted_a1[i][1]\n",
        "      for j in range(0,len(arr1)):\n",
        "        if x==arr1[j][1] and y==arr1[j][2]:\n",
        "          C.append(arr1[j][4])\n",
        "          break\n",
        "\n",
        "    for i in range(0,len(sorted_a2)):\n",
        "      x = sorted_a2[i][0]\n",
        "      y = sorted_a2[i][1]\n",
        "      for j in range(0,len(arr2)):\n",
        "        if x==arr2[j][0] and y==arr2[j][3]:\n",
        "          D.append(arr2[j][4])\n",
        "          break\n",
        "    print(\"EKHASNE SORTED THAKBE\")\n",
        "    print(C)\n",
        "    print(D)\n",
        "    val = levenshtein(C,D)\n",
        "    print(val)\n",
        "\n",
        "    per = (len(D)-val)/len(D)\n",
        "    per = per*100\n",
        "    print(per)\n",
        "\n",
        "\n",
        "    if (per==100.0):\n",
        "      correct_word_percentage[0]+=1\n",
        "    elif (per>=75.0):\n",
        "      correct_word_percentage[1]+=1\n",
        "    elif (per>=50.0):\n",
        "      correct_word_percentage[2]+=1\n",
        "    elif (per>=25.0):\n",
        "      correct_word_percentage[3]+=1\n",
        "    else:\n",
        "      correct_word_percentage[4]+=1\n",
        "    print(correct_word_percentage)"
      ],
      "metadata": {
        "id": "fjIOiKQGJ-67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Special Thank you to :\n",
        "M. S. Hasan and A. Pal, “Optical character recognition for handwritten bangla documents,” 2021, unpublished undergraduate thesis.\n",
        "\n",
        "In case of need, many code snippets were collected from Hasan et al's work and then upgraded. Authors will be forever grateful to them."
      ],
      "metadata": {
        "id": "q-Jqvl93PeWQ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}